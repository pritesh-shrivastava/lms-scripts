---
title: "De-duplicating CMS repository"
author: "Pritesh Shrivastava"
date: "January 22, 2018"
output:
  pdf_document: default
  html_document: default
  html_notebook: default
---

## Reading data from CMS

```{r warning=FALSE, message=FALSE}
library('tidyverse')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('stringr')
library('text2vec')

Qs <- read_tsv("qs_topicwise_dump.tsv")

head(Qs)
```

## Adding Grade, Subject, Curriculum and Chapter No 

```{r}
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
  dplyr::select(1:14) %>%                 # Keeping only the first 14 columns
  filter(!is.na(Difficulty))  %>%         # Cleaning overflow
  mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))

# Removing non-UTF characters
#Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
#Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
Qs_clean$fullText = paste(Qs_clean$Text, Qs_clean$A, Qs_clean$B, Qs_clean$C, Qs_clean$D)

Qs_clean <- Qs_clean %>% filter(str_length(Qs_clean$fullText) > 50)
# Removed Dummy Qs and Spectrum ans key

head(Qs_clean)

```

## Creating vocabulary by tokenizing text - using text2vec

```{r}
test <- Qs_clean %>% filter(Subject == "MTH") 
  
clean <- test$fullText %>%
    str_to_lower  %>% 
    {gsub("[^A-Za-z0-9]", " ", .)} %>%
    #{gsub("[^a-zA-Z\\s]", "", .)} %>% 
    #{gsub("//d", "",.)} %>%
    {gsub("[[:punct:]]"," ", .)} %>%
    str_squish



it = itoken(clean, progressbar = FALSE)

v = create_vocabulary(it) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 3)

```

## Creating Document Term Matices - text2vec

```{r}
vectorizer = vocab_vectorizer(v)

dtm = create_dtm(it, vectorizer)
dim(dtm)

```


### Applying tf-idf on dtm

```{r}
tfidf = TfIdf$new()
dtm_tfidf = fit_transform(dtm, tfidf)
dtm_tfidf[1:5, 1:5]
```

## Cosine similarities between all rows of dtm_tfidf matrix

```{r}
tfidf_cos_sim = sim2(x = dtm_tfidf, method = "cosine", norm = "l2")
tfidf_cos_sim[1:6, 1:6]

```

## Cosine similarity with Latent Semantic Analysis

Usually tf-idf/bag-of-words matrices contain a lot of noise. Applying LSA model can help with this problem, so you can achieve better quality similarities

```{r}
# Add no of topics in your repo (Keep topics less than 10 else code will crash)
lsa = LSA$new(n_topics = 10)
dtm_tfidf_lsa = fit_transform(dtm_tfidf, lsa)

tfidf_lsa_cos_sim = sim2(x = dtm_tfidf_lsa, method = "cosine", norm = "l2")
tfidf_lsa_cos_sim[1:6, 1:6]

```

### Tidying similarity matrix - VERY disk heavy operation. Crashed both EC2 and Ubuntu.
### Melt was even slower. Tidy did not change the matrix

```{r}
similarity <- tfidf_lsa_cos_sim %>% as.data.frame.table()
head(similarity)

```

### Filtering near duplicates

```{r}
# Figure out the optimum similarity threshold - 0.99,99,999
dupl_list <- filter(similarity, Freq > .9999999) %>%
  rename( row = Var1, column = Var2, Similarity_Measure = Freq) %>% 
  filter(as.numeric(row) > as.numeric(column))  %>%   # Remove double entries in a symmetric matrix i,j and j,i
  filter(row != column)       # Remove diagnol entries
  
head(dupl_list)
```

## Final list of Duplicate Qs

```{r}
duplicates <- dupl_list %>% 
  mutate(row_id = test$Code[row], row_q = test$fullText[row], row_topic = test$Topic_Code[row], row_status = test$Status[row],col_id = test$Code[column],  col_q = test$fullText[column],col_topic = test$Topic_Code[column], col_status = test$Status[column]) 

duplicates[c(1:5),c(4,5,7,8)]

write_csv(duplicates, "duplicate.csv")

```